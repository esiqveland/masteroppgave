\section{Voldemort}

\subsection{Configuration data}
In this section we will explore the essential configuration data used to manage Voldemort. It is split into two parts: global and local files. Global files are identical files hosted by each individual node in a cluster. These files contains persistent cluster metadata. These files needs to be consistent between nodes. As the number of nodes increase managing these files can become cumbersome. Local files typically contain per node specific persistent configuration data like hostname, ID and performance metrics. In our running cluster we use the following global configuration files:

\begin{itemize}
\item \texttt{cluster.xml} contains persistent information on all nodes involved in a cluster. All nodes must have an entry in this file before they can join the cluster. The fields in the file is relatively self explanatory. In addition to listing communication ports, each entry must have a unique ID and hostname. The partition field tells each node which parts of the keyspace it node is resposible for. In Voldemort each partition can be moved between nodes, however the number of partitions is not changeable after the cluster is running. In the example below the reader can verify that we have 16 partitions spread across 3 nodes. It is also possible to assign nodes into zones. Zones are typically used when operation in multiple data centers to prevent too much traffic between data centers. 

\begin{verbatim}
<cluster>
  <name>ntnucluster</name>
  <server>
    <id>0</id>
    <host>voldemort1.idi.ntnu.no</host>
    <http-port>8081</http-port>
    <socket-port>6666</socket-port>
    <admin-port>6667</admin-port>
    <rest-port>8085</rest-port>
    <partitions>4, 3, 8, 0, 13, 11</partitions>
  </server>
  <server>
    <id>1</id>
    <host>voldemort2.idi.ntnu.no</host>
    <http-port>8081</http-port>
    <socket-port>6666</socket-port>
    <admin-port>6667</admin-port>
    <rest-port>8085</rest-port>
    <partitions>7, 1, 6, 15, 9</partitions>
  </server>
  <server>
    <id>2</id>
    <host>voldemort3.idi.ntnu.no</host>
    <http-port>8081</http-port>
    <socket-port>6666</socket-port>
    <admin-port>6667</admin-port>
    <rest-port>8085</rest-port>
    <partitions>12, 2, 10, 14, 5</partitions>
  </server>
</cluster>
\end{verbatim}

\item \texttt{stores.xml} contains persistent metadata on all stores operated by a cluster. One cluster running Voldemort can have several pluggable stores. As with \texttt{cluster.xml}, each node must have it's own copy of this file. The parameters in this file control each store's behavior. 

The persistence field defines what kind of backend storage is used. Voldemort supports bdb, mySQL, memory, read-only, and cache. Cache and memory are both implementations that reside only in memory, the difference being how they react when out of storage space. It is possible to have multiple stores using different backend technology. 

Routing-strategy defines how replicas are stored in the cluster. Voldemort offers three alternative strategies: consistent routing, zone routing and all-routing. When using consistent routing all requests will be routed to the first N nodes from the keys location in the consistent hashing ring. Here N is the replication factor. Zone routing sits on top of consistent routing and ensures that each request goes to zone specific replicas. This is used to spread replicas across multiple data centers. Finally All-routing simply routes the request to all nodes specified by the call. 

As with routing, we also have 3 alternatives for hinted handoff strategy: any-handoff, consistent handoff and proximity handoff. When using any-handoff, a random live node in the cluster is selected for the request. With consistent hand-off enabled, one of the N nodes on the hash ring after the failed node will handle the request. Finally with proximity handoff requests will be routed according to the zone proximity of the clients zone id. This is useful if an entire data center is offline. 

We can also finely tune each individual cluster with regards to availability, consistency and durability. Replication factor specifies how many duplicates we want for each entry in the database. Required read and writes specifies how many nodes must respond to a request before it is considered successful. These three parameters greatly influences performance. 

Finally it is possible to specify the format on keys and values. Supported key formats are: json, java-serialization, string, protobuff, thrift and identity. Values can have the same formats and can be compressed with gzip or lzf. 

\begin{verbatim}
<stores>
    <store>
        <name>test</name>
        <persistence>bdb</persistence>
        <description>Test store</description>
        <owners>harry@hogwarts.edu, hermoine@hogwarts.edu</owners>
        <routing-strategy>consistent-routing</routing-strategy>
        <routing>client</routing>
        <hinted-handoff-strategy>consistent-handoff</hinted-handoff-strategy>
        <replication-factor>2</replication-factor>
        <required-reads>1</required-reads>
        <required-writes>1</required-writes>
        <key-serializer>
            <type>string</type>
        </key-serializer>
        <value-serializer>
            <type>string</type>
        </value-serializer>
    </store>
</stores>
\end{verbatim}

\item \texttt{local.properties} is used to configure individual nodes in a cluster. The node ID must map to an entry in \texttt{cluster.xml} for it to be valid. Using the maximum threads field one can tune the application running to the hardware of the individual node. This config file also contains store related login infomation used to access those. 

\begin{verbatim}
node.id=0
max.threads=100

############### DB options ######################
http.enable=true
socket.enable=true

# BDB
bdb.write.transactions=false
bdb.flush.transactions=false
bdb.cache.size=1G
bdb.one.env.per.store=true

# Mysql
mysql.host=localhost
mysql.port=1521
mysql.user=root
mysql.password=3306
mysql.database=test

#NIO connector settings.
enable.nio.connector=true
request.format=vp3
storage.configs=voldemort.store.bdb.BdbStorageConfiguration, voldemort.store.readonly.ReadOnlyStorageConfiguration
\end{verbatim}

\item We also have several local configuration files used during a rebalance operation. \texttt{rebalancing.steal.info.key} contains information on which partitions the node will need to fetch from other nodes in the system. The rebalance process also stores the existing \texttt{cluster.xml} and \texttt{stores.xml} before starting the rebalance operation in case of a roll back. Finally there is a \texttt{server.state} file that is used to persistently store rebalance state in case of a failure. It is either SERVER\_NORMAL or SERVER\_REBALANCING. 

\end{itemize}

% \subsection{cluster.xml}
% Cluster.xml is the file that contains persistent information about the nodes involved in the cluster. In order for a node to join the cluster it must first have an entry in this file. The fields in the file is relatively self explanatory. In addition to listing communication ports, each entry must have a unique ID and hostname. The partition field lists which parts of the keyspace this node will be resposible for. In Voldemort each partition can be moved between nodes, however the number of partitions is not changeable after the cluster is running. In the example below the reader can verify that we have 16 partitions spread across 3 nodes. It is also possible to group nodes into zones. These are typically used for grouping machines running in the same data-center. 




% \subsection{stores.xml}
% One running Voldemort cluster can have several pluggable stores. Metadata for these stores are located in the stores.xml file. This file must also be consistent between all nodes running in the cluster. The parameters in this file control each store's behavior. The persistence field defines what kind of backend storage that is used. Voldemort supports bdb, mySQL, memory, read-only, and cache. Cache and memory are both implementations that reside only in memory, the difference being how they react when out of storage space. 

% Routing-strategy defines how replicas are stored in the cluster. Voldemort offers three alternative strategies: consistent routing, zone routing and all-routing. When using consistent routing all requests will be routed to the first N nodes from the keys location in the consistent hashing ring. Here N is the replication factor. Zone routing sits on top of consistent routing and ensures that each request goes to zone specific replicas. This is used to spread replicas across multiple data centers. Finally All-routing simply routes the request to all nodes specified by the call. 

% As with routing, we also have 3 alternatives for hinted handoff strategy: any-handoff, consistent handoff and proximity handoff. When using any-handoff, a random live node in the cluster is selected for the request. With consistent hand-off enabled, one of the N nodes on the hash ring after the failed node will handle the request. Finally with proximity handoff requests will be routed according to the zone proximity of the clients zone id. This is useful if an entire data center is offline. 

% We can also finely tune each individual cluster with regards to availability, consistency and durability. Replication factor specifies how many duplicates we want for each entry in the database. Required read and writes specifies how many nodes must respond to a request before it is considered successful. These three parameters greatly influences performance. 

% Finally it is possible to specify the format on keys and values. Supported key formats are: json, java-serialization, string, protobuff, thrift and identity. Values can have the same formats and can be compressed with gzip or lzf. 





% \subsection{Local files}
% In addition to the previously mentioned global files, nodes running Voldemort also store some local files for individual configuration and persistent state. 

% \subsection{server.properties}

% \subsection{node.id}
% Persistant storage of this nodes id.

% \subsection{Rebalancing}
% When rebalancing some information is written to persistent storage in case the process fails and we need to roll back. This is stored in files prefixed by rebalancing.*. They contain information on which partitions each individual node will copy as well as the existing cluster.xml and store.xml. Each server also have a server state that is set during a rebalance. This state can be either NORMAL or REBALANCING. 


