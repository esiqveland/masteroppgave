\section{Voldemort}
\label{sec:voldemort}
In this section we will explore the essential configuration data used to manage Voldemort. It is split into two parts: global and local files. Global files are identical files hosted by each individual node in a cluster, but they also need to be consistent between all nodes. These files contains persistent cluster metadata. As the number of nodes increase managing these files can become cumbersome. Local files typically contain per-node specific persistent configuration data like hostname, ID and performance metrics. In our running cluster we use the following global configuration files:

\subsection{Configuration data}
Each node has at any point a full overview of all the other nodes in the cluster and what partitions they hold. This is necessary to always directly route a key to the correct node.
To provide this information, Voldemort distributes a XML file that describes the cluster, containing all hosts and partition information. This information is and needs to be consistent across all nodes.
Note that the partitioning is strictly speaking decoupled from the storage, leaving (N,R,W) as a per \emph{storage} setting.

Another feature of Voldemort is client side routing. Clients usually do routing of requests themselves, using the information of the cluster topology to find correct node for a key. A client fetches the cluster config when connecting, then uses this information for routing. When the client starts receiving errors of wrong routing from a or several nodes, it rereads the config to receive the updated information.

\begin{itemize}
\item \texttt{cluster.xml} contains persistent information on all nodes involved in a cluster. All nodes must have an entry in this file before they can join the cluster. The fields in the file is relatively self explanatory. In addition to listing communication ports, each entry must have a unique ID and hostname. The partition field tells each node which parts of the keyspace a node is responsible for. In Voldemort each partition can be moved between nodes, however the total number of partitions is not changeable after the cluster is running. In the example below the reader can verify that we have 16 partitions spread across 3 nodes. It is also possible to assign nodes into zones. Zones are typically used when operation in multiple data centers prevent too much traffic between data centers. 

\begin{verbatim}
<cluster>
  <name>ntnucluster</name>
  <server>
    <id>0</id>
    <host>voldemort1.idi.ntnu.no</host>
    <http-port>8081</http-port>
    <socket-port>6666</socket-port>
    <admin-port>6667</admin-port>
    <rest-port>8085</rest-port>
    <partitions>4, 3, 8, 0, 13, 11</partitions>
  </server>
  <server>
    <id>1</id>
    <host>voldemort2.idi.ntnu.no</host>
    <http-port>8081</http-port>
    <socket-port>6666</socket-port>
    <admin-port>6667</admin-port>
    <rest-port>8085</rest-port>
    <partitions>7, 1, 6, 15, 9</partitions>
  </server>
  <server>
    <id>2</id>
    <host>voldemort3.idi.ntnu.no</host>
    <http-port>8081</http-port>
    <socket-port>6666</socket-port>
    <admin-port>6667</admin-port>
    <rest-port>8085</rest-port>
    <partitions>12, 2, 10, 14, 5</partitions>
  </server>
</cluster>
\end{verbatim}

\item \texttt{stores.xml} contains persistent metadata on all stores operated by a cluster. One cluster running Voldemort can have several pluggable stores, served by different backends. These backends can even potentially be other databases, like a mysql or BDB database. As with \texttt{cluster.xml}, each node must have it's own copy of this file. The parameters in this file control each store's behavior. 

The persistence field defines what kind of backend storage is used. Voldemort supports bdb, mySQL, memory, read-only, and cache. Cache and memory are both implementations that reside only in memory, the difference being how they react when out of storage space. It is possible to have multiple stores using different backend technology. 

Routing-strategy defines how replicas are stored in the cluster. Voldemort offers three alternative strategies: consistent routing, zone routing and all-routing. When using consistent routing all requests will be routed to the first N nodes from the keys location in the consistent hashing ring. Here N is the replication factor. Zone routing sits on top of consistent routing and ensures that each request goes to zone-local replicas. This is used to spread replicas across multiple data centers. Finally All-routing simply routes the request to all nodes specified by the call. 

As with routing, we also have 3 alternatives for hinted handoff strategy: any-handoff, consistent handoff and proximity handoff. When using any-handoff, a random live node in the cluster is selected for the request. With consistent hand-off enabled, one of the N nodes on the hash ring after the failed node will handle the request. Finally with proximity handoff requests will be routed according to the zone proximity of the clients zone id. This is especially useful if an entire data center is offline. 

We can also finely tune each individual store with regards to availability, consistency and durability. Replication factor specifies how many duplicates we want for each entry in the database. Required read and writes specifies how many nodes must respond to a request before it is considered successful. These three parameters greatly influences performance. 

Finally it is possible to specify the format on keys and values. Supported key formats are: json, java-serialization, string, protobuff, thrift and identity. Values can have the same formats and can be compressed with gzip or lzf. 

\begin{verbatim}
<stores>
    <store>
        <name>test</name>
        <persistence>bdb</persistence>
        <description>Test store</description>
        <owners>harry@hogwarts.edu, hermoine@hogwarts.edu</owners>
        <routing-strategy>consistent-routing</routing-strategy>
        <routing>client</routing>
        <hinted-handoff-strategy>consistent-handoff</hinted-handoff-strategy>
        <replication-factor>2</replication-factor>
        <required-reads>1</required-reads>
        <required-writes>1</required-writes>
        <key-serializer>
            <type>string</type>
        </key-serializer>
        <value-serializer>
            <type>string</type>
        </value-serializer>
    </store>
</stores>
\end{verbatim}

\item \texttt{server.properties} is used to configure individual nodes in a cluster. The node ID must map to an entry in \texttt{cluster.xml} for it to be valid. Using the maximum threads field one can tune the application running to the hardware of the individual node. This config file also contains store related login infomation used to access those. 

\begin{verbatim}
node.id=0
max.threads=100

############### DB options ######################
http.enable=true
socket.enable=true

# BDB
bdb.write.transactions=false
bdb.flush.transactions=false
bdb.cache.size=2G
bdb.one.env.per.store=true

# Mysql
mysql.host=localhost
mysql.port=1521
mysql.user=root
mysql.password=3306
mysql.database=test

#NIO connector settings.
enable.nio.connector=true
request.format=vp3
storage.configs=voldemort.store.bdb.BdbStorageConfiguration, voldemort.store.readonly.ReadOnlyStorageConfiguration
\end{verbatim}

\item We also have several local configuration files used during a rebalance operation. \texttt{rebalancing.steal.info.key} contains information on which partitions the node will need to fetch from other nodes in the system. The rebalance process also stores the existing \texttt{cluster.xml} and \texttt{stores.xml} before starting the rebalance operation in case of a roll back. Finally there is a \texttt{server.state} file that is used to persistently store rebalance state in case of a failure. It is either \texttt{NORMAL\_SERVER} or \texttt{REBALANCING\_MASTER\_SERVER}. 

\end{itemize}

\subsection{Rebalancing}
In this section we will discuss how rebalancing is done in Voldemort. A rebalance operation involves repartitioning and moving data between nodes. There can be several reasons for rebalancing a cluster including adding nodes (cluster expansion), adding nodes to zones(zone expansion) and load balancing by shuffling partitions around. We would also like to be able to shrink the cluster (cluster contraction), however this is not yet supported by Voldemort. During a rebalance Voldemort suffers no down time and clients should not experience any noticeable performance drops. In addition there should be no loss or corruption of data if anything goes wrong and the rebalance is aborted. A rebalance is split into three steps which we will explain below. 

\subsubsection{Terminology}
Before we can go into details about the rebalance process we need to define a few terms:

\begin{itemize}
\item Stealer-node: When rebalancing this node will \emph{steal} a partition from another node and copy its data.  
\item Donor-node: When rebalancing this node will act as a donor and send data to one or more stealer nodes.
\item Donor-stealer pair: During a rebalance donors and stealers form pairs and copy data while proxying requests.
\item Task: A list of all patition-stores that must be cloned from the donor node.  
\item Zone n-ary: Closely tied to how many replicas there is of each data item. If we have a replication factor of 3 then the original entry is the Zone 0-ary, the first duplicate the zone 1-ary and the last one zone 2-ary. This information is used when deciding stealer-donor relationships during a rebalance. 
\item Proxy-bridge: When moving partitions during a rebalance, stealer-nodes are responsible for data they do not yet hold. To solve this, proxy bridges are put up between donor and stealer nodes so any request can be forwarded to a node holding the data.
\end{itemize}

\subsubsection{Preparation}
Before a rebalance can start, we need a plan. This plan contains which partitions to move to which nodes. When choosing where to move partitions, Voldemort has a set of design principles to assist in deciding where each node should copy its data from during a rebalance:

\begin{enumerate}
\item Only steal if you do not already host the required partitions
\item Try to steal from a node in the same zone, if this is not possible, steal from a donor in the same zone as the primary partition.
\item When you have to steal, steal from the same zone n-ary. 
\end{enumerate}

Following these principles the plan generated will limit data movement across zones in addition to aligning proxy-bridges with stealer-donor pairs. The output of such a planning operation is a file called \texttt{final\_cluster.xml}. This is a modified version of the \texttt{cluster.xml} and contains the newly proposed cluster setup. Executing the planning is done by running one of the rebalance* scripts provided with Voldemort. 

\begin{lstlisting}[style=customc, caption=Sample command to plan a cluster expansion. Outputs a \texttt{final\_cluster.xml} as well as a plan]
./bin/rebalance-cluster-expansion.sh -c current_cluster -s current_stores -i interim_cluster -o output dir
\end{lstlisting}

\subsubsection{Execution}
The first issue that needs to be handled when rebalancing is dealing with already connected clients. When a client connects it requests cluster metadata from Voldemort. The client uses this metadata for routing all future requests. When rebalancing, this metadata is no longer valid and clients must be informed. Voldemort has two ways of dealing with this issue. Either the client can check metadata version on each request or nodes can alert the client if they receive a request for a key they do not have responsibility for.  This alert is implemented by throwing an InvalidMetadataException. In the latter case clients only refresh their metadata after requesting a partition that has moved. 

Voldemort offers a proxy-pause option while rebalancing which is a window of time where the updated cluster metadata is propagated, and proxy-bridges are set up. This allows connected clients to request the new metadata before any actual rebalancing is done. The rebalance is executed using the supplied administrator tool. 

The steps involved in executing a rebalance are the following:

\begin{enumerate}
\item Upload interim metadata: To allow for a new node to join the cluster it must have an entry in \texttt{cluster.xml}. This is done by replacing \texttt{cluster.xml} with a modified \texttt{interim\_cluster.xml}. In this file an entry for the new node has been added. This node will not yet be responsible for any partitions.
\item Gather verification data: To later be able to verify that the rebalance was successful it is possible to extract some verification data for comparison before running the rebalance.
\item Stop asynchronous tasks: These tasks could cause the rebalance to abort.
\item Execute rebalance: Run the appropriate rebalance script.
\end{enumerate}

\begin{lstlisting}[style=customc, caption=Sample command to execute the rebalance. Parallelism defines how many tasks can be run at the same time.]
./bin/run-class.sh voldemort.tools.RebalanceControllerCLI --url \$URL  --final-cluster final-cluster.xml --parallelism 8 --proxy-pause 900
\end{lstlisting}

Servers will be notified of the rebalance by the client sending the new \texttt{cluster.xml} files and changing their state to REBALANCING\_MASTER\_SERVER. Servers will now start proxying requests according to the new partition setup. During the proxy-pause interval, nodes will pair up in donor-stealer pairs based on tasks and start proxying requests as needed. After the proxy-pause has ended, donors will start to send data to the stealers. Once all task has been completed proxy-bridges are torn down, server state changes back to NORMAL\_SERVER, and the cluster returns to normal operations.

\subsubsection{Verification}
After a rebalance is completed, it can be useful to verify that the rebalance was a success. There is also a repair job that should be run after the rebalance to delete any orphaned keys. Recall that the entries shared by a donor node are now orphans, and can be cleaned up by deleting them.

\begin{lstlisting}[style=customc, caption=Commands for pulling a key sample from a store and the versioned data objects stored under the keys. These are used to verify that the rebalance did not corrupt data]
./bin/run-class.sh voldemort.utils.KeySamplerCLI --url $BOOTSTRAP_URL --out-dir key-samples --records-per-partition 5
./bin/run-class.sh voldemort.utils.KeyVersionFetcherCLI --url $URL --in-dir key-samples --out-dir key-version-fetches
\end{lstlisting}

\begin{lstlisting}[style=customc, caption=Sample repair job script. Passing -1 as node will run the script on all nodes.]
./bin/voldemort-admin-tool.sh --repair-job --node '-1' --url \$URL
\end{lstlisting}

\subsubsection{Recovery from failure}
A benefit of having a strict key-value store, is that there are no advanced feature. No invariants that must be kept or avoided, no joins, foreign keys, searches or other indexes (except for the primary, the key) that complicates moving and partitioning of data. 
Think of Voldemort as a giant, distributed and persistent hash map.
This simplifies the process for performing rebalancing, ie. scaling a lot. We can simply copy the partition, with some precautions to ensure durability and integrity.

When scaling, we give a list of $(node, partition) \rightarrow node$ pairs. The left side is the FROM node and right side TO, the destination node.
A list of such pairs is called a \emph{rebalance plan}, a set of transfers that are to be executed during the rebalance.
The first part of a secure transfer, is the setup of proxy bridges. This starts the routing of request to the \emph{new} layout. ie. if we are moving $(node0, 3) \rightarrow node1$, all requests that are going to partition 3, are now routed to the new node, $node1$. 

After allowing proxy settings to propagate for an interval, the receiving node, the \emph{stealer}, initiates a request to transfer the target partition from the \emph{donor}. 
Now to allow for a safe transfer, all PUTs to node1 are also sent (proxyed) back to node0. This ensures no writes are lost in case of an abort or other failure. All GETs are first looked up locally in case they already have been transferred. If not, the key is fetched explicitly from the donor and returned.
In case of failures, we abort the plan and set the routing back to the old configuration.

It is easy to see that this set of rules allows a partition move to be canceled or crash at any time without incurring data loss.


% There are several factors that was important for us when choosing which distributed key-value store to work with. We both had detailed theoretical knowledge of Dynamo after holding a presentation on their paper. The simple design and data model of Voldemort also was a good fit for our project as we did not have any application specific requirements for our storage system to meet. Lastly we wanted to work with an open source project. Voldemort fulfilled all these requirements. 

% Voldemort keep its configuration data in several XML files. These files reside on each individual node in the cluster. The process of executing a repartition or rebalance involves another set of xml files as well as various scripts that must be run. As the number of nodes grows this number of configuration files can get out of hand quickly. 
% As a result, creating a system to handle these configuration files was proposed as a ``fun project'' on the project-voldemort website. Apache ZooKeeper was proposed as a possible service for this system.  







% \subsection{cluster.xml}
% Cluster.xml is the file that contains persistent information about the nodes involved in the cluster. In order for a node to join the cluster it must first have an entry in this file. The fields in the file is relatively self explanatory. In addition to listing communication ports, each entry must have a unique ID and hostname. The partition field lists which parts of the keyspace this node will be resposible for. In Voldemort each partition can be moved between nodes, however the number of partitions is not changeable after the cluster is running. In the example below the reader can verify that we have 16 partitions spread across 3 nodes. It is also possible to group nodes into zones. These are typically used for grouping machines running in the same data-center. 




% \subsection{stores.xml}
% One running Voldemort cluster can have several pluggable stores. Metadata for these stores are located in the stores.xml file. This file must also be consistent between all nodes running in the cluster. The parameters in this file control each store's behavior. The persistence field defines what kind of backend storage that is used. Voldemort supports bdb, mySQL, memory, read-only, and cache. Cache and memory are both implementations that reside only in memory, the difference being how they react when out of storage space. 

% Routing-strategy defines how replicas are stored in the cluster. Voldemort offers three alternative strategies: consistent routing, zone routing and all-routing. When using consistent routing all requests will be routed to the first N nodes from the keys location in the consistent hashing ring. Here N is the replication factor. Zone routing sits on top of consistent routing and ensures that each request goes to zone specific replicas. This is used to spread replicas across multiple data centers. Finally All-routing simply routes the request to all nodes specified by the call. 

% As with routing, we also have 3 alternatives for hinted handoff strategy: any-handoff, consistent handoff and proximity handoff. When using any-handoff, a random live node in the cluster is selected for the request. With consistent hand-off enabled, one of the N nodes on the hash ring after the failed node will handle the request. Finally with proximity handoff requests will be routed according to the zone proximity of the clients zone id. This is useful if an entire data center is offline. 

% We can also finely tune each individual cluster with regards to availability, consistency and durability. Replication factor specifies how many duplicates we want for each entry in the database. Required read and writes specifies how many nodes must respond to a request before it is considered successful. These three parameters greatly influences performance. 

% Finally it is possible to specify the format on keys and values. Supported key formats are: json, java-serialization, string, protobuff, thrift and identity. Values can have the same formats and can be compressed with gzip or lzf. 





% \subsection{Local files}
% In addition to the previously mentioned global files, nodes running Voldemort also store some local files for individual configuration and persistent state. 

% \subsection{server.properties}

% \subsection{node.id}
% Persistant storage of this nodes id.

% \subsection{Rebalancing}
% When rebalancing some information is written to persistent storage in case the process fails and we need to roll back. This is stored in files prefixed by rebalancing.*. They contain information on which partitions each individual node will copy as well as the existing cluster.xml and store.xml. Each server also have a server state that is set during a rebalance. This state can be either NORMAL or REBALANCING. 


