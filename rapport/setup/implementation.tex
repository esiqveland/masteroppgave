% implementation.tex

Our work and implementation happened mainly in 3 parts. Part 1 consists of integrating ZooKeeper support and mechanisms into a Voldemort server. Making Voldemort read configurations and storing data in ZooKeeper. Part 2 contains our work towards rebalancing by the help of ZooKeeper. In Part 3 we create the service Headmaster, which is a process to run along side of the Voldemort process. Headmaster listens to active nodes in the cluster, and can generate configuration for new nodes, add them to the cluster, and prepare the members for a rebalance to include the new node.

\section{Part 1}
Integrating ZooKeeper into Voldemort and taking advantage of the coordination services ZooKeeper provides, proved rather difficult.
We chose to override and rewrite much of the current MetadataStore implementation to one that uses ZooKeeper as the backend storage engine instead of local files. 
Actual data is still of course stored locally, but configuration (meta data) now resides in the ZooKeeper cluster.
This abstraction works rather well for shared data files, but causes some issues with information that is stored for individual nodes. 
As such, we landed on a fixed path scheme, where certain configuration files are expected to reside on known locations, and will be presented later.

This change introduces the dependency of a running ZooKeeper cluster for normal operation of any node. While this is a liability and potential source for downtime in a highly available system, ZooKeeper is itself a highly available system and the 5 node setup is regarded as very stable and in heavy use at Yahoo\ref{need:citation}.
TODO: CITATION

Alternatively we could have made the coordination service optional, but we then considered it risky to allow updating config files live through ZooKeeper. In any case, the system will in general be able to operate through short ZooKeeper outages, as all meta and config data is cached locally, however changes will not propagate to the affected nodes as quickly.

\subsection{Configuration}
Configuration is stored in a VoldemortConfig object. Normally this is created from settings stored in local files per node. At startup, the code looks for a properties file in a directory given by environment variable \texttt{VOLDEMORT\_HOME}. 

Now, if you specify a ZooKeeper connection URL as a command line parameter, the program looks for configuration in ZooKeeper.
We have given an example outline of the expected directory structure in Figure \ref{fig:configdirs}. This is for the three nodes \texttt{vold0.idi.ntnu.no}, \texttt{vold1.idi.ntnu.no} and \texttt{vold2.idi.ntnu.no}. You can see the expected path follows the pattern \texttt{/config/HOSTNAME/server.properties}. 

The startup code then reads the config file and tries to parse. If parsing fails, the server fails to clearly signal something is wrong at startup, and needs immediate fixing. If the file is not found, the server registers a watch on the path in ZooKeeper. If there is a write to this file, ZooKeeper will let the server know and the config file can be re-read. 

One can argue whether to fail or listen when a config is not found, but we decided for the latter, to listen for changes after implementing the management service, as we will later see it allows us to push new config files to a node by the help of our management daemon Headmaster. 

\begin{figure}[h]
\label{fig:configdirs}
\dirtree{%
.1 / \ldots{} \begin{minipage}[t]{8cm}(root directory)
			  \end{minipage}.
.2 config.
.3 nodes \ldots{} (individual node configs).
.4 vold0.idi.ntnu.no \ldots{} (hostname).
.5 server.properties.
.4 vold1.idi.ntnu.no.
.5 server.properties.
.4 vold2.idi.ntnu.no.
.5 server.properties.
}
\caption{Individual node configs in the shared space.}
\end{figure}

Our modification inherits from VoldemortConfig, so that the ZooKeeper capable version can be used in the same code, in the same manner. The modified code reads data from a fixed location in ZooKeeper, and sets the necessary settings and properties for the server to bootstrap. The ZooKeeper connection URL is passed as a command line parameter.

The directory structure for the whole program can be viewed in its entirety in Figure \ref{fig:dirstruct}, and has been included for completeness.

\begin{figure}[h]
\label{fig:dirstruct}
\dirtree{%
.1 / \ldots{} \begin{minipage}[t]{8cm}(The root can also be a chroot{.})
			  \end{minipage}.
.2 active.
.3 vold0.idi.ntnu.no.
.3 vold2.idi.ntnu.no.
.2 config.
.3 nodes (individual node configs).
.4 vold0.idi.ntnu.no.
.4 vold1.idi.ntnu.no.
.4 vold2.idi.ntnu.no.
.3 cluster.xml.
.3 stores.xml.
.2 headmaster.
.3 headmaster\_0009.
.3 headmaster\_0011.
.3 headmaster\_0012.
}
\caption{Directory structure in ZooKeeper. The nodes or directories are actually Znodes and can hold data.}
\end{figure}

\subsection{MetadataStore}
The internal state for a node is written to a Store called MetadataStore. It consists of an in memory cache store, and a persistent one to disk. 

\section{Handling ZooKeeper connection}


\section{Part 2}

\subsection{Initial configuration}


\subsection{Rebalance}
Another issue we bumped into is related to the architecture of Voldemort.
In a rebalance, the new config files are pushed (written) to each node separately using Voldemort admin data requests. This causes each node to write the new data given to the MetadataStore.
Because we would like to be notified about changes to the config using watches, each write causes N watches to trigger, where N is the number of nodes in the Voldemort cluster.
A write therefore also causes N watches to trigger, and N new watches to be set. So for a full configuration rewrite in a rebalance, N*N re-reads and watches would be set and reset, and also N writes in quick succession.
We therefore ignores such put requests in the ZooKeeper driven MetadataStore, and just put the new data in the Metadata cache, deferring the admin to use a ZooKeeper write operation to put the new config out, making the nodes do a re-read.


\subsection{Begrepsavklaring}
A \texttt{write} or \texttt{put} in the context of ZooKeeper is considered a ZooKeeper \texttt{setData} operation.

A ZooKeeper event, is an event triggered by a watch set in ZooKeeper, giving push information about a change in ZooKeeper about a znode being watched.

In the context of ZooKeeper, the word file might be used for a znode. File used in the context of a local filesystem is a local file.


