% intro.tex

%In this chapter we will present our findings and evaluate what impact the added features have on a working node.
In this chapter we will present the data we have collected from our measurements and how they were taken.

While management and monitoring can be both useful and convenient, it does introduce overhead. This overhead should not have a significant impact on system performance, others citing as low as 1-2\% impact as tolerable.

Here we will verify the impact of our work on the overall throughput and performance of the software under different workloads. We will look at requests processed per second and response time distribution under typical usage patterns and some edge cases. 

We will first look at single node performance, with and without our modifications.
Next we will try verifying that performance scaling is intact. For Voldemort, near linear scaling of throughput is expected when adding worker nodes to the cluster.
Lastly we will look at performance during a automatically triggered rebalance under load.

\section{The tests}
All tests are run on a clean cluster instance, with a warm up period loading data into the database. All tests are run three times, with the average score used.

Test0:

100\% read ?
100\% read no mod ?

99\% 1\% read/update ? read/write?
99\% 1\% read/update ? read/write? no mod

1\%/99\% read/write ?
1\%/99\% read/write ? no mod

linear scaling:
100\% read ? 2 nodes
100\% read no mod ? 2 nodes

99\% 1\% read/update ? read/write? 2 nodes
99\% 1\% read/update ? read/write? no mod | 2 nodes

These are the most realistic workloads for Voldemort, as a high volume read only service and about 98-99\% read to write/update.

rebalance:
Just monitor request rate during a rebalance.
Remember to somewhat record size of data set.

\begin{center}
\begin{table}[h]
	\begin{tabular}{|c|c|c|l|}
		\multicolumn{1}{c}{Test case} & 
		\multicolumn{1}{c}{read \%} & 
		\multicolumn{1}{c}{write \%} & 
		\multicolumn{1}{c}{state} \\
		\hline

		TC0 & 100 & 0 & no mod \\
		TC1 & 100 & 0 & mod \\
		TC2 & 99 & 1 & no mod \\
		TC3 & 99 & 1 & mod \\
		TC4 & 1 & 99 & no mod \\
		TC5 & 1 & 99 & mod \\

		\hline
	\end{tabular}
	\caption{Test case setup. No mod/mod is shorthand for no modification and modification, meaning whether we will be using original or our source code for this test.}
	\label{tbl:testcases}
\end{table}
\end{center}

\section{Experiences}
Networkspeed is a limitation. 
No real difference between 12MB and 2G cache. 


Findings:

Network is of the utmost importance.

Performance seem very consistent.


\input{results/figures}


\section{Experiments}
\subsection{Setup}
\subsubsection{Benchmark tool}
We use Voldemorts built in benchmark tool to send data to our cluster. This allows for varying request distribution, frequency and number of sending threads. TODO: Eivind fyll inn med scripts osv... 

\subsubsection{Hardware}
We have 4 computers involved in these experiments:
\begin{enumerate}
\item Hackintosh: Intel Core i7 2.5GHz Quad Core  16 GB Memory 250GB SSD
\item MacBook Pro: Intel Core i7 2.6 GHz Quad Core 8 GB Memory 250GB SSD
\item MacBook Pro: Intel Core i5 2.5 GHz Dual Core 8 GB Memory 120GB SSD
\item Mac Mini: Intel Core 2 Duo 2.4 GHz Dual Core 8 GB Memory 250GB SSD
\end{enumerate}

In addition we use a D-Link Dir-655 gigabit router for networking.

\subsubsection{Software}
All computers run Apple OS X 10.9.2 operating system. 

\subsection{Initial benchmarking}
In this part we want to investigate how our computer and cluster behaves in various scenarios. We will in all experiments use a workload of 95\% read and 5\% write requests as is common on a typical social website. Each data entry is 1kb. 


\subsubsection{Response time distribution}
We want to establish a baseline for how each individual node performs with regards to servicing a request. In this experiment we will setup single computer Voldemort instance on each computer and measure the distribution of service time. We will run this test with both our ZooKeeper implementation and the original Voldemort code. We will send queries as fast as possible.

We expect little difference in results as all nodes should run below maximum capacity. We expect majority of these requests to be serviced in under 1 ms as found by Rabl et al.\cite{vldb2012}

\subsubsection{Performance of single nodes}
We also want to get an idea of each individual nodes ability to service queries. To test this we will again run single node clusters and send requests as fast as possible.

With our workload and entry size we have a theoretical limit of around 60 000 requests per second as this is equal to 125MB or 1Gbit data per second. We expect the 4 cores of the i7 to be able to serve up to 60 000 requests, while the two computers will perform considerably worse. 

\subsection{Experiments on the cluster}
In this part of the experiments we have a look at how a cluster consisting of several nodes performs. We will explore different scenarios both with and without rebalancing.

\subsubsection{Baseline}
In this experiment we want to establish a baseline for how a cluster behaves under load. We have both MacBooks sharing 16 partitions. We define node 1 to be the Core i7 MacBook and node 2 to be then Core i5 Macbook.  We use the Hackintosh for generating traffic. 

We expect node 1 to be able to serve its requests with ease while node 2 will struggle with near 100\% utilization of the CPU. As this is not an optimal work distribution we expect measured throughput to be lower than under optimal conditions. As in the case with the initial benchmarks, we will run the tests both on our ZooKeeper implementation and the original Voldemort project. 

\subsubsection{Adaptive cluster experiment}
In this experiment we want to see how the cluster behaves when we allow a struggling node to move partitions over on a node under less stress. Node 1 and 2 are set up the same way as in the baseline experiment. A struggling node is defined as a node running at less than 20\% idle CPU.

We expect throughput to be lower during the rebalance phase, but overall we will see an increase in performance over time as we gain a more optimal distribution of partition. 

We will again run the experiment on both our implementation and the original Voldemort implementation to see if there is any major differences in performance. On the original implementation we trigger rebalances manually until we reach a state where no node is stressed. 

After viewing the results we found a severe bug in our ZooKeeper implementation. Which led us to redo this test on our implementation. More on this in the results section. We chose to include the incorrect result for completeness. 


\subsubsection{Mini and slowpro with fastpro to the rescue}
In this experiment we will utilize all our features by adding a node to a already struggling cluster and automatically move partitions to alleviate the existing nodes. We will have a cluster consisting of the Mac Mini and i5 MacBook Pro and we will add the i7 Macbook Pro during the experiment. 

We expect the system to perform poorly during the initial phase as both computers will be running at near max capacity. After we add third computer and move the first partition the performance should increase. As the system migrates more partitions away from the struggling nodes the overall performance should keep increasing. 

\subsubsection{Partitions containing lots of data. See how much throughput is affected. Different rebalance strategies}





\section{Results}

\subsection{Initial benchmarking}
Below we present our results








