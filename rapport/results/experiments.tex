% intro.tex

%In this chapter we will present our findings and evaluate what impact the added features have on a working node.

In this chapter we will present benchmarks we have run and their corresponding results. We will benchmark both the original Voldemort implementation and our own to compare the impact of our work on the overall throughput and performance of the system. We will look at requests processed per second, response time distribution and CPU-utilization under a typical usage pattern. 

Our experiments are done in two phases. First part is a single-node benchmark of each node that will participate in the tests. This is to get an understanding of their performance. As our cluster consists of very different hardware it will be useful to have an idea of what to expect from each individual node before we add them together.
The second part concerns performance and scaling after and during rebalancing of a cluster. The rebalance process will be executed both manually and using our automatic system for balancing.

The goal of these experiments is to verify that after our modifications the performance remains intact. For Voldemort, near linear scaling of throughput is expected when adding worker nodes to the cluster. 

\section{Setup}
We will here present the setup and tools used for our testing.

\subsection{Benchmark tool}
We use Voldemorts provided benchmark tool to send data to our cluster. The program is heavily based on the work by Yahoo Cloud Serving Benchmark\cite{ycsb}. This allows us to generate different types of workloads, request loads and number of sending threads. We also had to do some modifications to get a running average throughput, instead of a total average throughput when measuring.

We parse most of the logs in python to transform the data points into something useful. In some cases we also have to manually put the data together.
For analysis, we mainly use \texttt{R}\cite{Rproject} with extensive use of the \texttt{Hmisc}\cite{Hmisc} library. Our Rscripts can be found in our project on github\cite{githubproject}. All the diagrams in this chapter has been generated with \texttt{R}.

An example run with the benchmark tool could be:
\begin{lstlisting}[language=bash]
read_percent=95
write_percent=5
metric=histogram # [summary|histogram]
ops=1 000 000 # total requests to run
threads=54
recordcount=1 000 000 # records to insert during warmup
valuesize=1024 # bytes per record

voldemort-performance-tool.sh --threads $threads --metric-type $metric 
--ops-count $ops --url $url --value-size $valuesize 
--store-name $store --record-count $recordcount $@
\end{lstlisting}

These are also the settings we used for most of our tests.

\subsection{Hardware}
We have 4 computers involved in these experiments:

\begin{enumerate}
	\item Custom built desktop: Intel Core i7 3.5 GHz Quad Core  16 GB Memory 250GB SSD
	\item MacBook Pro: Intel Core i7 2.6 GHz Quad Core 8 GB Memory 250GB SSD
	\item MacBook Pro: Intel Core i5 2.5 GHz Dual Core 8 GB Memory 120GB SSD
	\item Mac Mini: Intel Core 2 Duo 2.4 GHz Dual Core 8 GB Memory 250GB SSD
\end{enumerate}

In addition we use a D-Link Dir-655 gigabit router for networking.

\subsection{Software}
All computers run Apple OS X 10.9.2 operating system and Java JDK 1.7.0\_51. 

\section{Single-node benchmarking}
To verify our modifications are not too costly for performance, we will be concentrating on two metrics during our tests. The response time distribution has been included to verify \emph{how fast} we are answering most queries. The throughput metric has been included to verify whether raw, overall work performance is intact, ie. \emph{how many} queries we can answer.

In this part we want to investigate how our computer and cluster behaves in various scenarios. We will in all experiments use a workload of 95\% read and 5\% write requests as is common on a typical social website where most browse with a few comments or status updates. Each data entry is 1kB. The database is seeded with 1 million entries before each test starts, unless otherwise specified. In all tests the i7 desktop is generating requests. 

\subsection{Response time distribution}
Response time is an important metric and the response time distribution is very helpful in determining perceived performance. Response time is important for both user experience when using a service and for upholding SLAs.

We want to establish a baseline for how each individual node performs with regards to time used responding per requests. This is to verify the impact our modifications have on the users experience of performance. In this experiment we will setup a single node Voldemort instance, and measure the service time over a given number of requests. We will run this test with both our ZooKeeper implementation and the official Voldemort code for comparison. 
Queries are generated as fast as possible to test response time at maximum load, i.e. the worst case.

We expect comparable results between the nodes, as all nodes will run at maximum capacity. We also expect the majority of these requests to be serviced in under 1 ms as found by Rabl et al.\cite{Rabl:2012:SBD:2367502.2367512}.

\subsubsection{Throughput of single nodes}
We also want to get an overview over each nodes ability to serve requests. To test throughput we will again run single node clusters and send requests at maximum load. The throughput is measured over 1 million requests, for values of 1024 bytes. 

As our hardware is so different we expect vast difference in individual node performance. The old core 2 duo will not fare well compared to the i5 and i7. 

\subsection{Adaptive cluster}
In these experiments we have a look at how a cluster consisting of several nodes performs. We will explore different scenarios both with and without rebalancing, and look at how throughput and CPU usage is affected. For these experiments we increase the preloaded values in the system to 3 million to get a bigger data set to move during each rebalance operation. 

\subsubsection{Baseline}
This experiment will act as a base line for how a cluster behaves under load. We have 2 nodes sharing 16 partitions, 8 partitions each. We run this experiment on the two slowest nodes to make increases in performance more obvious. As we only have 2 nodes in the cluster we only use one required read / write on each request and no data duplication. Recall that this is a \texttt{(1,1,1)} system if we refer to the (N,R,W) parameters of Voldemort.

We expect both nodes to struggle with servicing their workload and as a result the throughput should be less than optimal. In other words we do not expect the throughput of these two nodes combined to be equal to the sum of their individual throughputs. 

\subsubsection{Manual rebalancing}
In this experiment we want to see how the cluster behaves when we allow a struggling node to move partitions over to a node under less stress. We will manually move one partition at the time over from a struggling node to one less worked. We will use the same criteria as in our automatic system ,which means that a struggling node is defined as a node running over 85\% utilized CPU. We will continue to move partitions until there are no nodes under 85\% utilization. This experiment is run on the original Voldemort code. 

During rebalancing we expect the throughput to suffer somewhat, however we expect to see an increase in performance over time as we gain a more optimal distribution of partitions. 

\subsubsection{Automatic rebalancing}
This experiment is a rerun of the previous one except there is no manual interaction and Headmaster is calling all the shots.

We expect this experiment to play out the same way as the previous one, with comparable performance.

\subsubsection{Automatic cluster expansion}
In this test we will utilize all our features by adding a node to an already struggling cluster and automatically move partitions to alleviate the existing nodes. We will have a cluster consisting of the Mac Mini and i5 MacBook Pro and we will add the i7 Macbook Pro during the experiment. Headmaster will be in charge of triggering rebalance as needed. 

We expect the system to perform poorly during the initial phase as both computers will be running at near max capacity. After we add third computer and move the first partition the performance should increase. As the system migrates more partitions away from the struggling nodes the overall performance should keep increasing until it can keep up with maximum workload. 

\subsubsection{Large scale move of partitions}
Finally we want to investigate how much throughput suffers during a rebalance. We will increase the size of the data set to 15 million records to have a longer rebalance stage. This will also make sure that the entire work set does not fit in memory. We run this as a stand alone test because the other tests moving data too quickly to yield enough data points for proper analysis. 

As the entire work set does not fit in memory, we expect throughput to be lower than in the other tests. During a rebalance the throughput be consistent but lower than before the rebalance took place. We expect the second test to perform worse as more data is being transfered in parallel. 










