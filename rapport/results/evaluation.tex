% evaluation.tex

We will first mention a few of the general experiences we got doing this project.
We will then discuss the response time results. In Section \ref{eval:throughput} we will discuss the single node throughputs.
Section \ref{eval:balance} discusses the results of the rebalance operations and measured performance scaling.

\section{Experiences}
\subsubsection{Network limits}
We did not have access to a lot of different hardware while doing this project. Our computers proved quite fast, with each individual node pushing 50-70 MB per second. This made us quickly reach the network speed limit in our benchmarking system, receiving over 130MB/s second from the cluster while benchmarking. This limit was reached around 60k requests/s, so this is our practical, and theoretical, maximum throughput of 1024 kB values. On a 100mbit connection we could only send or receive about 5k requests/s, or roughly 12 MB/s. 

\subsubsection{Server side caching}
While testing, we noticed no real difference between a 12MB and 2GB database cache on our Voldemort servers. This might suggest there is a problem with the cache setting in the software. All of our hardware run on SSDs, but we still consider this result surprising. There could be more work done here on largely different sizes of data sets, but this was not our main focus.

\subsubsection{Client updates}
during rebalancing, we found a critical bug we had introduced in the way clients receive updates of configuration.

\subsubsection{VectorClock of Stores}

\subsubsection{Other notes}
Overall we found the performance to be very consistent. Our results were on large very reproducible and saw little variance between benchmark runs.

\section{Response time}
During these tests we see a lot of different behavior for the different hardware. This would suggest there can be a lot to gain on better balancing between heterogeneous nodes.


The i7 with 4 cores clearly does the best, displaying same performance for both code bases and sub 5 ms response time for 99.9\% of requests.

TODO: include?
On our initial tests we alse have found no significant difference in the performance between our ZooKeeper implementation and the original Voldemort code. Contrary to our expectations, the response time distribution varies greatly on different hardware. In our results the 0.999 percentile response time varies from 250 ms on the slowest hardware to 5 ms on the fastest. On the .90 percentile there is however is much less difference with response times of 2 ms on the slowest hardware and 1 ms on the fastest.

\section{Throughput}
\label{eval:throughput}
When it comes to the single node throughput benchmarks the results are more in line with what we expected. The slow Core 2 Duo is barely able to serve 17000 requests per second while the Core i5 and i7 are able to serve 32000 and 44000. With our theoretical limit of 60 000 requests per second we can expect a cluster consisting of the two slower machines to be unable to service a full workload, while a cluster with all 3 nodes should comfortably do so. This information is useful for structuring our adaptive cluster experiments.

\section{Scaling and balancing}
\label{eval:balance}
As we can see, to achieve full potential scaling some skewing of partitions is required. In our adaptive cluster experiments we have compared automated to manual rebalancing and found our automatic solution to perform on par with the manual one. We have also done a extensive cluster expansion test and monitored CPU-utilization and throughput. These experiments show that our implementation works as intended.

Figure shows that this can also be achieved by automatic methods.

