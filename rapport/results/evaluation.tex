% evaluation.tex
This chapter will cover discussion regarding our results. In Section \ref{eval:results} we discuss our results from the experiments. Section \ref{eval:discussion} covers our thoughts on the system we have created as a whole and in Section \ref{eval:exp} we share some general experiences we had during the project.

\section{Results discussion}
\label{eval:results}
In this section we will discuss our results as well as issues related to network limits.  

\subsection{Network limits}
We did not have access to different network hardware while doing this project. Our computers proved very capable, with each individual node pushing 34-70 MB per second in raw network data. This made us quickly reach the network speed limit of our gigabit network infrastructure. At this point the load generator was receiving over 130MB/s from the cluster. This limit was reached at around 60k requests per second each of 1024 kB values, providing a hard limit for our throughput. This is why throughput is capped at 60k in our experiments, even though there still are nodes with lots of idle CPU-cycles. On a 100Mbit connection we could only send or receive about 5k requets per second equaling to around 12 MB/s

\subsection{Response time distribution and throughput}
\label{eval:performance}
In our initial benchmarking experiments we have found no significant difference in the performance when comparing our ZooKeeper implementation with the original Voldemort code. In some cases our implementation is performing slightly better, while in other the opposite is true. We believe this mostly comes down to variance.

Contrary to our expectations, the response time distribution varies greatly on different hardware. In our results the 0.999 percentile response time varies from 250 ms on the slowest hardware to 5 ms on the fastest. On the .90 percentile there is however is much less difference with response times of 2 ms on the slowest hardware and 1 ms on the fastest.

Single-node throughput results are more in line with what we expected. The slow Core 2 Duo is barely able to serve 17k requests per second while the Core i5 and i7 are able to serve 32k and 44k. 

\subsection{Scaling and balancing}
\label{eval:balance}
To achieve full potential scaling some skewing of partitions is required. In our adaptive cluster experiments we have compared automated to manual rebalancing and found our automatic solution to perform on par with the manual one. We have also done a extensive cluster expansion test and monitored CPU-utilization and throughput. These experiments show that our implementation works as intended. 

With optimal partition distribution, a cluster consisting of the two slowest machines was able to service 46k requests per second providing almost the sum of both individual nodes. We also see that a cluster of all three nodes with proper partition distribution easily was able to service the maximum request rate of 60k requests per second. 

In our final experiments we see how partition-size affects performance loss during a rebalance. Our results show that loss of performance is closely tied to how big of \% keys are affected by the rebalance. 

Before starting we were worried of getting into scenario where two or more nodes ends up endlessly juggling partitions. We deemed this the biggest weakness of such a automated system. This situation did however never occur in our testing. We still think this is something that potential users need to be aware of, and need to watch out for. To mitigate this behavior, having a high number of partitions to minimize performance impact of moving one, seem like a good option.

Having a high number of partitions have been a point we have made several times in this thesis. There is however a draw back. If there is a very high number of partitions in the system, and not a lot of data, we could have situations were moving a partition has little to no impact on the target systems. This would lead to waste of network and system resources for little to no gain. This could be somewhat mitigated by moving several partitions when we are first moving, but we consider this to be more error prone. 



\subsection{Other notes}
Overall we found the performance to be very consistent. Our results were on large very reproducible and saw little variance between benchmark runs.


\section{System evaluation}
\label{eval:discussion}
Overall we are happy with our results. They show that we have not noticeably affected the performance of Voldemort when adding automatic scaling. For now the monitor service is started as a separate process on each node involved in Voldemort. This could be incorporated into Voldemort, but we yet to do so.  

For now Headmaster is implemented as a standalone service so that it also can be run remotely, and independently from Voldemort.
While working, we had at one point inadvertently started two Headmasters at once, which kept fighting for control.
We spent a fair amount of time debugging weird race conditions until we realized there was two services operating at the same time.
It is now a master based service with leader election through ZooKeeper that can run on any number of nodes, without fighting for control.

Our system is best used in diverse clusters of heterogeneous nodes where Headmaster can help automatically distribute the partition set across different nodes to improve cluster load balance. As the performance loss while rebalancing is not too bad, it could be possible to utilize Headmasters rebalancing during peak hours to alleviate struggling nodes. This of course is heavily dependent on partition size. A large amount of partitions will give smaller amounts of data to move, but if each partition is only held by a single node, nothing can be done even if you have many nodes and partitions. They are just too large to move in this scenario. It is therefore of significant importance to assign enough partitions for a huge data set when first creating the cluster.

Another point to be made is that Headmaster is \emph{standalone}. Even with our code, the only dependency for any operation or function is an available ZooKeeper instance. The cluster still works fine even if Headmaster is down, not working or crashed.

Our work should be rather failure tolerant, but we have not done very extensive failure scenario testing, there might still be some bugs. 

We feel that all our goals are met with the system we have created.

\section{General Experiences}
\label{eval:exp}
In this section we cover general experiences we have made and problems we encountered that we find worth sharing. 

\subsubsection{Server side caching}
While testing, we noticed no real difference in performance between a 12MB and 2GB database cache on our Voldemort servers. This might suggest there is a problem with the cache setting in Voldemort. All of our hardware run on SSDs, but we would still consider this result surprising. This could be investigated further using largely different of data sets, but this was not our main focus in this thesis. However, a theory is that the data files fit into the OS file caching, still providing very good read speeds.

\subsubsection{Client updates}
While benchmarking rebalancing we found a critical bug we had introduced in the way clients receive updates of configuration. It caused values being moved during a rebalance to be unavailable for the entire rebalance duration. This would have been hard to discover if we did not compare with the original Voldemort implementation. A graph illustration the issue is available in Figure: \ref{fig:adaptive_bug}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{results/throughput/adaptive/zookeeper/auto_2nodes_error}
    \caption{Fatal bug causing data moved during a rebalance to be inaccessable for close to the entire duration of the rebalance, causing major drops in throughput.}
    \label{fig:adaptive_bug}
\end{figure}

\subsubsection{VectorClock of Stores}
We also had issues with translating a config files version number in ZooKeeper to a valid vector clock in Voldemort. When a config file is committed into Voldemort internally, it is supposed to follow the same rules for vector clock versioning as normal keys.
The bug caused the rebalance cluster.xml and stores.xml sent to nodes when getting ready for a rebalance to be rejected, failing the rebalance attempt.
The bug was also only discovered after the first rebalance in a single session, so it took some time until we encountered it.

It is related to how the StoreDefinition object is versioned internally and was quite hard to track down. When the first rebalance occurs, the new StoreDefinition is saved to the \emph{cache} only, with a incremented version clock. When the next rebalance occurs, the StoreDefinition is fetched from the persistent store. This version does not include the vector clock from the previous one stored in the cache, so when the new StoreDefinition is saved, the vector clock has the same or an old value and is rejected by the store.

We circumvented this by making sure we fetch StoreDefinitions from the cache only when we are rebalancing, preserving the vector clock between runs.


